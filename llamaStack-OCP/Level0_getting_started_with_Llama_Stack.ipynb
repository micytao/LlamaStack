{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce5ac225-ad97-4cbd-8e90-f0ad64cbf00b",
   "metadata": {},
   "source": [
    "# Level 0: Getting Started with Llama Stack\n",
    "\n",
    "This notebook will help you set up your environment for this tutorial. Specifically, we will cover installing the necessary libraries, configuring essential parameters, and connecting to a Llama Stack server.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "\n",
    "Ensure you have access to a [Llama Stack](https://llama-stack.readthedocs.io/en/latest/) server.\n",
    "\n",
    "If you need to set one up, please follow the instruction set below that is appropriate for your environment:\n",
    "\n",
    "* [Local](../../../local_setup_guide.md) setup guide for a laptop.\n",
    "* [Remote](../../../kubernetes/llama-stack/README.md) setup guide for an OpenShift cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf06a5b-7259-4e4b-94ef-fbb577c30f9e",
   "metadata": {},
   "source": [
    "## Installing Dependencies\n",
    "\n",
    "This code requires `llama-stack` and the `llama-stack-client` python packages. Let's begin by installing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba0ddc8b-d6cf-4cb3-8678-7b52ee70131e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies Installed\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt &> /dev/null  & echo \"Dependencies Installed\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589b1ae-9fd5-4715-a630-cc3cc52035e9",
   "metadata": {},
   "source": [
    "## Setting the Environment Variables\n",
    "\n",
    "Rename or copy the [`.env.example`](../../../.env.example) file to create a new file called `.env`. We've included as many reasonable defaults as possible to get you started, but please use this file to make any customizations needed for your environment such as the the location of the Llama Stack server endpoint or your personal [Tavily](https://app.tavily.com) api key for web search.  \n",
    "\n",
    "```bash\n",
    "cp .env.example .env\n",
    "```\n",
    "\n",
    "### Environment variables required for all demos\n",
    "- `REMOTE_BASE_URL`: the URL of the remote Llama Stack server.\n",
    "- `TEMPERATURE` (optional): the temperature to use during inference. Defaults to 0.0.\n",
    "- `TOP_P` (optional): the top_p parameter to use during inference. Defaults to 0.95.\n",
    "- `MAX_TOKENS` (optional): the maximum number of tokens that can be generated in the completion. Defaults to 512.\n",
    "- `STREAM` (optional): set this to True to stream the output of the model/agent and False otherwise. Defaults to False.\n",
    "- `VDB_PROVIDER`: the vector DB provider to be used. Must be supported by Llama Stack. For this demo, we use Milvus Lite which is our preferred solution.\n",
    "- `VDB_EMBEDDING`: the embedding model to be used for ingestion and retrieval. For this demo, we use all-MiniLM-L6-v2.\n",
    "- `VDB_EMBEDDING_DIMENSION` (optional): the dimension of the embedding. Defaults to 384.\n",
    "- `VECTOR_DB_CHUNK_SIZE` (optional): the chunk size for the vector DB. Defaults to 512.\n",
    "- `REMOTE_OCP_MCP_URL`: the URL for your Openshift MCP server. If the client does not find the tool registered to the llama-stack instance, it will use this URL to register the Openshift tool.\n",
    "- `REMOTE_SLACK_MCP_URL`: the URL for your Slack MCP server. If the client does not find the tool registered to the llama-stack instance, it will use this URL to register the Slack tool.\n",
    "- `USE_PROMPT_CHAINING`: dictates if the prompt should be formatted as a few separate prompts to isolate each step or in a single turn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573899bf-34cb-4f31-9fc1-48aca439fc60",
   "metadata": {},
   "source": [
    "## Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66309122-7bea-4c25-8822-388b4db3c253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/app-root/lib64/python3.11/site-packages (1.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting llama-stack==0.2.6\n",
      "  Downloading llama_stack-0.2.6-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting blobfile (from llama-stack==0.2.6)\n",
      "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting fire (from llama-stack==0.2.6)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: httpx in /opt/app-root/lib64/python3.11/site-packages (from llama-stack==0.2.6) (0.28.1)\n",
      "Collecting huggingface-hub (from llama-stack==0.2.6)\n",
      "  Downloading huggingface_hub-0.32.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: jinja2>=3.1.6 in /opt/app-root/lib64/python3.11/site-packages (from llama-stack==0.2.6) (3.1.6)\n",
      "Requirement already satisfied: jsonschema in /opt/app-root/lib64/python3.11/site-packages (from llama-stack==0.2.6) (4.23.0)\n",
      "Collecting llama-stack-client>=0.2.6 (from llama-stack==0.2.6)\n",
      "  Downloading llama_stack_client-0.2.7-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting openai>=1.66 (from llama-stack==0.2.6)\n",
      "  Downloading openai-1.82.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: prompt-toolkit in /opt/app-root/lib64/python3.11/site-packages (from llama-stack==0.2.6) (3.0.50)\n",
      "Requirement already satisfied: python-dotenv in /opt/app-root/lib64/python3.11/site-packages (from llama-stack==0.2.6) (1.1.0)\n",
      "Collecting pydantic>=2 (from llama-stack==0.2.6)\n",
      "  Downloading pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib64/python3.11/site-packages (from llama-stack==0.2.6) (2.32.3)\n",
      "Requirement already satisfied: rich in /opt/app-root/lib64/python3.11/site-packages (from llama-stack==0.2.6) (13.9.4)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib64/python3.11/site-packages (from llama-stack==0.2.6) (75.8.2)\n",
      "Requirement already satisfied: termcolor in /opt/app-root/lib64/python3.11/site-packages (from llama-stack==0.2.6) (2.3.0)\n",
      "Collecting tiktoken (from llama-stack==0.2.6)\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: pillow in /opt/app-root/lib64/python3.11/site-packages (from llama-stack==0.2.6) (11.1.0)\n",
      "Collecting h11>=0.16.0 (from llama-stack==0.2.6)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: kubernetes in /opt/app-root/lib64/python3.11/site-packages (from llama-stack==0.2.6) (30.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib64/python3.11/site-packages (from jinja2>=3.1.6->llama-stack==0.2.6) (3.0.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/app-root/lib64/python3.11/site-packages (from llama-stack-client>=0.2.6->llama-stack==0.2.6) (4.9.0)\n",
      "Requirement already satisfied: click in /opt/app-root/lib64/python3.11/site-packages (from llama-stack-client>=0.2.6->llama-stack==0.2.6) (8.1.8)\n",
      "Collecting distro<2,>=1.7.0 (from llama-stack-client>=0.2.6->llama-stack==0.2.6)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib64/python3.11/site-packages (from llama-stack-client>=0.2.6->llama-stack==0.2.6) (2.2.3)\n",
      "Collecting pyaml (from llama-stack-client>=0.2.6->llama-stack==0.2.6)\n",
      "  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: sniffio in /opt/app-root/lib64/python3.11/site-packages (from llama-stack-client>=0.2.6->llama-stack==0.2.6) (1.3.1)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib64/python3.11/site-packages (from llama-stack-client>=0.2.6->llama-stack==0.2.6) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/app-root/lib64/python3.11/site-packages (from llama-stack-client>=0.2.6->llama-stack==0.2.6) (4.12.2)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib64/python3.11/site-packages (from httpx->llama-stack==0.2.6) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/app-root/lib64/python3.11/site-packages (from httpx->llama-stack==0.2.6) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/app-root/lib64/python3.11/site-packages (from httpx->llama-stack==0.2.6) (3.10)\n",
      "INFO: pip is looking at multiple versions of httpcore to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting httpcore==1.* (from httpx->llama-stack==0.2.6)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.66->llama-stack==0.2.6)\n",
      "  Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2->llama-stack==0.2.6)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2->llama-stack==0.2.6)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2->llama-stack==0.2.6)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pycryptodomex>=3.8 (from blobfile->llama-stack==0.2.6)\n",
      "  Downloading pycryptodomex-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /opt/app-root/lib64/python3.11/site-packages (from blobfile->llama-stack==0.2.6) (1.26.20)\n",
      "Collecting lxml>=4.9 (from blobfile->llama-stack==0.2.6)\n",
      "  Downloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: filelock>=3.0 in /opt/app-root/lib64/python3.11/site-packages (from blobfile->llama-stack==0.2.6) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub->llama-stack==0.2.6) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub->llama-stack==0.2.6) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub->llama-stack==0.2.6) (6.0.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub->llama-stack==0.2.6)\n",
      "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema->llama-stack==0.2.6) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema->llama-stack==0.2.6) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema->llama-stack==0.2.6) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/app-root/lib64/python3.11/site-packages (from jsonschema->llama-stack==0.2.6) (0.23.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/app-root/lib64/python3.11/site-packages (from kubernetes->llama-stack==0.2.6) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/app-root/lib64/python3.11/site-packages (from kubernetes->llama-stack==0.2.6) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/app-root/lib64/python3.11/site-packages (from kubernetes->llama-stack==0.2.6) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/app-root/lib64/python3.11/site-packages (from kubernetes->llama-stack==0.2.6) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/app-root/lib64/python3.11/site-packages (from kubernetes->llama-stack==0.2.6) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/app-root/lib64/python3.11/site-packages (from kubernetes->llama-stack==0.2.6) (3.2.2)\n",
      "Requirement already satisfied: wcwidth in /opt/app-root/lib64/python3.11/site-packages (from prompt-toolkit->llama-stack==0.2.6) (0.2.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests->llama-stack==0.2.6) (3.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/app-root/lib64/python3.11/site-packages (from rich->llama-stack==0.2.6) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/app-root/lib64/python3.11/site-packages (from rich->llama-stack==0.2.6) (2.19.1)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->llama-stack==0.2.6)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/app-root/lib64/python3.11/site-packages (from google-auth>=1.0.1->kubernetes->llama-stack==0.2.6) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/app-root/lib64/python3.11/site-packages (from google-auth>=1.0.1->kubernetes->llama-stack==0.2.6) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/app-root/lib64/python3.11/site-packages (from google-auth>=1.0.1->kubernetes->llama-stack==0.2.6) (4.9)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/app-root/lib64/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->llama-stack==0.2.6) (0.1.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama-stack-client>=0.2.6->llama-stack==0.2.6) (2.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama-stack-client>=0.2.6->llama-stack==0.2.6) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama-stack-client>=0.2.6->llama-stack==0.2.6) (2025.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/app-root/lib64/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes->llama-stack==0.2.6) (0.6.1)\n",
      "Downloading llama_stack-0.2.6-py3-none-any.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading llama_stack_client-0.2.7-py3-none-any.whl (292 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading openai-1.82.0-py3-none-any.whl (720 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.4/720.4 kB\u001b[0m \u001b[31m372.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m370.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
      "Downloading huggingface_hub-0.32.0-py3-none-any.whl (509 kB)\n",
      "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m331.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m226.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Downloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m225.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodomex-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m241.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m382.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114299 sha256=0d3731a540b2679e6cd79e258064c20b0a3dfa5c6ad3512de78fdf1f240bc59a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-dhpvtzz9/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
      "Successfully built fire\n",
      "Installing collected packages: typing-inspection, regex, pydantic-core, pycryptodomex, pyaml, lxml, jiter, hf-xet, h11, fire, distro, annotated-types, tiktoken, pydantic, huggingface-hub, httpcore, blobfile, openai, llama-stack-client, llama-stack\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.21\n",
      "    Uninstalling pydantic-1.10.21:\n",
      "      Successfully uninstalled pydantic-1.10.21\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.7\n",
      "    Uninstalling httpcore-1.0.7:\n",
      "      Successfully uninstalled httpcore-1.0.7\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "codeflare-sdk 0.27.0 requires pydantic<2, but you have pydantic 2.11.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 blobfile-3.0.0 distro-1.9.0 fire-0.7.0 h11-0.16.0 hf-xet-1.1.2 httpcore-1.0.9 huggingface-hub-0.32.0 jiter-0.10.0 llama-stack-0.2.6 llama-stack-client-0.2.7 lxml-5.4.0 openai-1.82.0 pyaml-25.1.0 pycryptodomex-3.23.0 pydantic-2.11.5 pydantic-core-2.33.2 regex-2024.11.6 tiktoken-0.9.0 typing-inspection-0.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install python-dotenv\n",
    "! pip install llama-stack==0.2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "043b421b-fb8f-4ae0-8f71-83a2a4114084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gencoder\n",
      "  Downloading gencoder-0.2.2-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Downloading gencoder-0.2.2-py2.py3-none-any.whl (3.6 kB)\n",
      "Installing collected packages: gencoder\n",
      "Successfully installed gencoder-0.2.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install gencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b09511e-f6dc-4a54-b037-d64adbeaa8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.types import UserMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb65ed-f368-4bb5-957b-6698fe85b829",
   "metadata": {},
   "source": [
    "## Setting Up the Server Connection\n",
    "\n",
    "Establish the connection to your Llama Stack server.\n",
    "\n",
    "_Note: A Tavily search API key is required for some of our demos and must be provided to the client upon initialization. If you do not have one, you can set one up for free at https://app.tavily.com_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d60fb3f3-4d04-4916-84fc-f798b059ff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server\n"
     ]
    }
   ],
   "source": [
    "base_url = os.getenv(\"REMOTE_BASE_URL\", \"http://llamastack-server:8321\")\n",
    "\n",
    "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
    "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "if tavily_search_api_key is None:\n",
    "    provider_data = None\n",
    "else:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "\n",
    "print(f\"Connected to Llama Stack server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f5ae8-8083-4896-a973-f310df129ec6",
   "metadata": {},
   "source": [
    "## Initializing the Inference Parameters\n",
    "\n",
    "Fetch the inference-related parameters from the corresponding environment variables and convert them to the format Llama Stack expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0092359-a5db-4d9a-a735-bb931ba05f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Parameters:\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 512}\n",
      "\tstream: False\n"
     ]
    }
   ],
   "source": [
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cdd34a",
   "metadata": {},
   "source": [
    "Now, let's use the Llama stack inference API to greet our LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e4423b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm an artificial intelligence and don't have feelings, but I'm here to help you. How can I assist you today?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = UserMessage(\n",
    "    content=\"Hi, how are you?\",\n",
    "    role=\"user\",\n",
    ")\n",
    "client.inference.chat_completion(\n",
    "    model_id=\"granite32-8b\",\n",
    "    messages=[message],\n",
    "    sampling_params=sampling_params,\n",
    "    stream=stream\n",
    ").completion_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d0a375",
   "metadata": {},
   "source": [
    "# Next\n",
    "\n",
    "Now that we've set up our Tutorial environment, Let's get started building with Llama Stack! The next notebook will teach you how to build a [Simple RAG](./Level1_simple_RAG.ipynb) application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1219179",
   "metadata": {},
   "source": [
    "#### Any Feedback?\n",
    "\n",
    "If you have any feedback on this or any other notebook in this demo series we'd love to hear it! Please go to https://www.feedback.redhat.com/jfe/form/SV_8pQsoy0U9Ccqsvk and help us improve our demos. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
